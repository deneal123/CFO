{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40ae3cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NightMare\\anaconda3\\envs\\envGPU\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\NightMare\\anaconda3\\envs\\envGPU\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "C:\\Users\\NightMare\\anaconda3\\envs\\envGPU\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.23-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "# Loading Dependencies\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import pandas as pd\n",
    "import transformers\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from tokenizers import BertWordPieceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db5fe89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train1 = pd.read_csv('C:/Users/NightMare/Desktop/neurofeed_back/train_df.csv')\n",
    "valid = pd.read_csv('C:/Users/NightMare/Desktop/neurofeed_back/valid_df.csv')\n",
    "test  = pd.read_csv('C:/Users/NightMare/Desktop/neurofeed_back/test_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8921e9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_encode(texts, tokenizer, chunk_size=256, maxlen=512):\n",
    "    \"\"\"\n",
    "    Encoder for encoding the text into sequence of integers for BERT Input\n",
    "    \"\"\"\n",
    "    all_ids = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(texts), chunk_size)):\n",
    "        text_chunk = texts[i:i+chunk_size].tolist()\n",
    "        encs = tokenizer.encode_batch(text_chunk)\n",
    "        all_ids.extend([enc.ids for enc in encs])\n",
    "    \n",
    "    return np.array(all_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5aa20b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "\n",
    "# Configuration\n",
    "EPOCHS = 3\n",
    "BATCH_SIZE = 16\n",
    "MAX_LEN = 192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83bcf00a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tokenizer(vocabulary_size=119547, model=BertWordPiece, unk_token=[UNK], sep_token=[SEP], cls_token=[CLS], pad_token=[PAD], mask_token=[MASK], clean_text=True, handle_chinese_chars=True, strip_accents=None, lowercase=False, wordpieces_prefix=##)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First load the real tokenizer\n",
    "tokenizer = transformers.DistilBertTokenizer.from_pretrained('distilbert-base-multilingual-cased')\n",
    "# Save the loaded tokenizer locally\n",
    "tokenizer.save_pretrained('.')\n",
    "# Reload it with the huggingface tokenizers library\n",
    "fast_tokenizer = BertWordPieceTokenizer('vocab.txt', lowercase=False)\n",
    "fast_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2cef0e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 253/253 [00:01<00:00, 171.95it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (64646,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m x_train \u001b[38;5;241m=\u001b[39m \u001b[43mfast_encode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfast_tokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxlen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMAX_LEN\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m x_valid \u001b[38;5;241m=\u001b[39m fast_encode(valid\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m), fast_tokenizer, maxlen\u001b[38;5;241m=\u001b[39mMAX_LEN)\n\u001b[0;32m      3\u001b[0m x_test \u001b[38;5;241m=\u001b[39m fast_encode(test\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m), fast_tokenizer, maxlen\u001b[38;5;241m=\u001b[39mMAX_LEN)\n",
      "Cell \u001b[1;32mIn[3], line 12\u001b[0m, in \u001b[0;36mfast_encode\u001b[1;34m(texts, tokenizer, chunk_size, maxlen)\u001b[0m\n\u001b[0;32m      9\u001b[0m     encs \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode_batch(text_chunk)\n\u001b[0;32m     10\u001b[0m     all_ids\u001b[38;5;241m.\u001b[39mextend([enc\u001b[38;5;241m.\u001b[39mids \u001b[38;5;28;01mfor\u001b[39;00m enc \u001b[38;5;129;01min\u001b[39;00m encs])\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_ids\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (64646,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "x_train = fast_encode(train1.text.astype(str), fast_tokenizer, maxlen=MAX_LEN)\n",
    "x_valid = fast_encode(valid.text.astype(str), fast_tokenizer, maxlen=MAX_LEN)\n",
    "x_test = fast_encode(test.astype(str), fast_tokenizer, maxlen=MAX_LEN)\n",
    "\n",
    "y_train = train1.label.values\n",
    "y_valid = valid.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea85625",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((x_train, y_train))\n",
    "    .repeat()\n",
    "    .shuffle(2048)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(AUTO)\n",
    ")\n",
    "\n",
    "valid_dataset = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((x_valid, y_valid))\n",
    "    .batch(BATCH_SIZE)\n",
    "    .cache()\n",
    "    .prefetch(AUTO)\n",
    ")\n",
    "\n",
    "test_dataset = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices(x_test)\n",
    "    .batch(BATCH_SIZE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5418e223",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(transformer, max_len=512):\n",
    "    \"\"\"\n",
    "    function for training the BERT model\n",
    "    \"\"\"\n",
    "    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    sequence_output = transformer(input_word_ids)[0]\n",
    "    cls_token = sequence_output[:, 0, :]\n",
    "    out = Dense(1, activation='sigmoid')(cls_token)\n",
    "    \n",
    "    model = Model(inputs=input_word_ids, outputs=out)\n",
    "    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6507d2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_layer = (\n",
    "    transformers.TFDistilBertModel\n",
    "    .from_pretrained('distilbert-base-multilingual-cased')\n",
    ")\n",
    "model = build_model(transformer_layer, max_len=MAX_LEN)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab45e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = x_train.shape[0] // BATCH_SIZE\n",
    "train_history = model.fit(\n",
    "    train_dataset,\n",
    "    steps_per_epoch=n_steps,\n",
    "    validation_data=valid_dataset,\n",
    "    epochs=EPOCHS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c37e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = x_valid.shape[0] // BATCH_SIZE\n",
    "train_history_2 = model.fit(\n",
    "    valid_dataset.repeat(),\n",
    "    steps_per_epoch=n_steps,\n",
    "    epochs=EPOCHS*2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062d2852",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = model.predict(test_dataset, verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
