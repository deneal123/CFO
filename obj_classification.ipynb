{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b143dd85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NightMare\\anaconda3\\envs\\envGPU\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\NightMare\\anaconda3\\envs\\envGPU\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "C:\\Users\\NightMare\\anaconda3\\envs\\envGPU\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.23-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d540eb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=0, alpha=0, size_average=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        if isinstance(alpha,(float,int)): self.alpha = torch.Tensor([alpha,1-alpha])\n",
    "        if isinstance(alpha,list): self.alpha = torch.Tensor(alpha)\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        if input.dim()>2:\n",
    "            input = input.view(input.size(0),input.size(1),-1)  # N,C,H,W => N,C,H*W\n",
    "            input = input.transpose(1,2)    # N,C,H*W => N,H*W,C\n",
    "            input = input.contiguous().view(-1,input.size(2))   # N,H*W,C => N*H*W,C\n",
    "        target = target.view(-1,1)\n",
    "\n",
    "        logpt = F.log_softmax(input)\n",
    "        logpt = logpt.gather(1,target)\n",
    "        logpt = logpt.view(-1)\n",
    "        pt = Variable(logpt.data.exp())\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            if self.alpha.type()!=input.data.type():\n",
    "                self.alpha = self.alpha.type_as(input.data)\n",
    "            at = self.alpha.gather(0,target.data.view(-1))\n",
    "            logpt = logpt * Variable(at)\n",
    "\n",
    "        loss = -1 * (1-pt)**self.gamma * logpt\n",
    "        if self.size_average: return loss.mean()\n",
    "        else: return loss.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d518c87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "\n",
    "  def __init__(self, texts, targets, tokenizer, max_len=512):\n",
    "    self.texts = texts\n",
    "    self.targets = targets\n",
    "    self.tokenizer = tokenizer\n",
    "    self.max_len = max_len\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.texts)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    text = str(self.texts[idx])\n",
    "    target = self.targets[idx]\n",
    "\n",
    "    encoding = self.tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=self.max_len,\n",
    "        return_token_type_ids=False,\n",
    "        padding='max_length',\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt',\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "    return {\n",
    "      'text': text,\n",
    "      'input_ids': encoding['input_ids'].flatten(),\n",
    "      'attention_mask': encoding['attention_mask'].flatten(),\n",
    "      'targets': torch.tensor(target, dtype=torch.long)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce754414",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClassifier:\n",
    "\n",
    "    def __init__(self, model_name, model_path, tokenizer_path, model_save_path, path_to_plot, n_classes=2, epochs=1):\n",
    "        self.model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(tokenizer_path)\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model_save_path=model_save_path\n",
    "        self.max_len = 512\n",
    "        self.epochs = epochs\n",
    "        self.out_features = self.model.bert.encoder.layer[1].output.dense.out_features\n",
    "        self.model.classifier = torch.nn.Linear(self.out_features, n_classes)\n",
    "        self.model.to(self.device)\n",
    "        self.path_to_plot = path_to_plot\n",
    "        self.current_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "        self.model_name = model_name\n",
    "    \n",
    "    def preparation(self, X_train, y_train, X_valid, y_valid):\n",
    "        # create datasets\n",
    "        self.train_set = CustomDataset(X_train, y_train, self.tokenizer)\n",
    "        self.valid_set = CustomDataset(X_valid, y_valid, self.tokenizer)\n",
    "\n",
    "        # create data loaders\n",
    "        self.train_loader = DataLoader(self.train_set, batch_size=1, shuffle=True)\n",
    "        self.valid_loader = DataLoader(self.valid_set, batch_size=1, shuffle=True)\n",
    "\n",
    "        # helpers initialization\n",
    "        self.optimizer = AdamW(self.model.parameters(), lr=2e-5, correct_bias=False)\n",
    "        self.scheduler = get_linear_schedule_with_warmup(\n",
    "                self.optimizer,\n",
    "                num_warmup_steps=0,\n",
    "                num_training_steps=len(self.train_loader) * self.epochs\n",
    "            )\n",
    "\n",
    "        # Обновляем веса функции потерь\n",
    "        self.loss_fn = FocalLoss()\n",
    "            \n",
    "    def fit(self):\n",
    "        self.model = self.model.train()\n",
    "        losses = []\n",
    "        correct_predictions = 0\n",
    "\n",
    "        # Wrap the train_loader with tqdm for progress bar\n",
    "        for data in tqdm(self.train_loader, desc='Training'):\n",
    "            input_ids = data[\"input_ids\"].to(self.device)\n",
    "            attention_mask = data[\"attention_mask\"].to(self.device)\n",
    "            targets = data[\"targets\"].to(self.device)\n",
    "\n",
    "            outputs = self.model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "\n",
    "            preds = torch.argmax(outputs.logits, dim=1)\n",
    "            loss = self.loss_fn(outputs.logits, targets)\n",
    "\n",
    "            correct_predictions += torch.sum(preds == targets)\n",
    "\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "            self.optimizer.step()\n",
    "            self.scheduler.step()\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "        train_acc = correct_predictions.double() / len(self.train_set)\n",
    "        train_loss = np.mean(losses)\n",
    "        return train_acc, train_loss\n",
    "    \n",
    "    def eval(self):\n",
    "        self.model = self.model.eval()\n",
    "        losses = []\n",
    "        correct_predictions = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Wrap the valid_loader with tqdm for progress bar\n",
    "            for data in tqdm(self.valid_loader, desc='Evaluation'):\n",
    "                input_ids = data[\"input_ids\"].to(self.device)\n",
    "                attention_mask = data[\"attention_mask\"].to(self.device)\n",
    "                targets = data[\"targets\"].to(self.device)\n",
    "\n",
    "                outputs = self.model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask\n",
    "                )\n",
    "\n",
    "                preds = torch.argmax(outputs.logits, dim=1)\n",
    "                loss = self.loss_fn(outputs.logits, targets)\n",
    "                correct_predictions += torch.sum(preds == targets)\n",
    "                losses.append(loss.item())\n",
    "\n",
    "        val_acc = correct_predictions.double() / len(self.valid_set)\n",
    "        val_loss = np.mean(losses)\n",
    "        return val_acc, val_loss\n",
    "    \n",
    "    def train(self):\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        train_accuracies = []\n",
    "        val_accuracies = []\n",
    "        \n",
    "        # Генерация уникального имени файла на основе времени\n",
    "        unique_filename = f\"weights_{self.model_name}_{self.current_time}.pt\"\n",
    "        save_path = os.path.join(self.model_save_path, unique_filename)\n",
    "\n",
    "        best_accuracy = 0\n",
    "        for epoch in range(self.epochs):\n",
    "            print(f'Epoch {epoch + 1}/{self.epochs}')\n",
    "            train_acc, train_loss = self.fit()\n",
    "            print(f'Train loss {train_loss} accuracy {train_acc}')\n",
    "            train_losses.append(train_loss)\n",
    "            train_accuracies.append(train_acc.item())\n",
    "\n",
    "            val_acc, val_loss = self.eval()\n",
    "            print(f'Val loss {val_loss} accuracy {val_acc}')\n",
    "            val_losses.append(val_loss)\n",
    "            val_accuracies.append(val_acc.item())\n",
    "            print('-' * 10)\n",
    "\n",
    "            if val_acc > best_accuracy:\n",
    "                torch.save(self.model, save_path)\n",
    "                best_accuracy = val_acc\n",
    "    \n",
    "        self.model = torch.load(save_path)\n",
    "\n",
    "        # Сохраняем график обучения\n",
    "        self.save_training_plot(train_losses, val_losses, train_accuracies, val_accuracies)\n",
    "\n",
    "    def save_training_plot(self, train_losses, val_losses, train_accuracies, val_accuracies):\n",
    "        epochs = range(1, self.epochs + 1)\n",
    "\n",
    "        plt.figure(figsize=(10, 5))\n",
    "\n",
    "        # Построение графика потерь\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(epochs, train_losses, 'b', label='Бинарная\\nкроссэнтропия\\nна тренировке')\n",
    "        plt.plot(epochs, val_losses, 'r', label='Бинарная\\nкроссэнтропия\\nна валидации')\n",
    "        plt.title('Функция потерь бинарная кроссэнтропия (loss)')\n",
    "        plt.xlabel('Эпохи')\n",
    "        plt.ylabel('Значение функции потерь')\n",
    "        plt.legend()\n",
    "\n",
    "        # Построение графика точности\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(epochs, train_accuracies, 'b', label='Точность\\nна тренировке')\n",
    "        plt.plot(epochs, val_accuracies, 'r', label='Точность\\nна валидации')\n",
    "        plt.title('Метрика точность (accuracy)')\n",
    "        plt.xlabel('Эпохи')\n",
    "        plt.ylabel('Точность')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Сохранение значений в JSON файл\n",
    "        json_data = {\n",
    "            \"train_losses\": train_losses,\n",
    "            \"val_losses\": val_losses,\n",
    "            \"train_accuracies\": train_accuracies,\n",
    "            \"val_accuracies\": val_accuracies\n",
    "        }\n",
    "\n",
    "        json_filename = f\"data_{self.model_name}_{self.current_time}.json\"\n",
    "        json_save_path = os.path.join(self.path_to_plot, json_filename)\n",
    "\n",
    "        with open(json_save_path, 'w') as json_file:\n",
    "            json.dump(json_data, json_file)\n",
    "        \n",
    "        # Генерация уникального имени файла на основе времени\n",
    "        unique_filename = f\"plot_{self.model_name}_{self.current_time}.png\"\n",
    "        save_path = os.path.join(self.path_to_plot, unique_filename)\n",
    "\n",
    "        # Сохранение графика в указанную директорию\n",
    "        plt.savefig(save_path, dpi=1000)\n",
    "\n",
    "        # Отображение графика\n",
    "        plt.show()\n",
    "    \n",
    "    def predict(self, text):\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        \n",
    "        out = {\n",
    "              'text': text,\n",
    "              'input_ids': encoding['input_ids'].flatten(),\n",
    "              'attention_mask': encoding['attention_mask'].flatten()\n",
    "          }\n",
    "        \n",
    "        input_ids = out[\"input_ids\"].to(self.device)\n",
    "        attention_mask = out[\"attention_mask\"].to(self.device)\n",
    "        \n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids.unsqueeze(0),\n",
    "            attention_mask=attention_mask.unsqueeze(0)\n",
    "        )\n",
    "        \n",
    "        prediction = torch.argmax(outputs.logits, dim=1).cpu().numpy()[0]\n",
    "\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35c0c22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('C:/Users/NightMare/Desktop/neurofeed_back/Preprocessdata/train_osn/test_obj_23.csv')\n",
    "valid_data = pd.read_csv('C:/Users/NightMare/Desktop/neurofeed_back/Preprocessdata/train_df_obj0.csv')\n",
    "test_data  = pd.read_csv('C:/Users/NightMare/Desktop/neurofeed_back/Preprocessdata/valid_df_obj0.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c01b7e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cointegrated/rubert-tiny2 were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "classifier = BertClassifier(\n",
    "        model_name='rubert-tiny2',\n",
    "        model_path='cointegrated/rubert-tiny2',\n",
    "        tokenizer_path='cointegrated/rubert-tiny2',\n",
    "        model_save_path='C:/Users/NightMare/Desktop/neurofeed_back/weights',\n",
    "        path_to_plot='C:/Users/NightMare/Desktop/neurofeed_back/plot',\n",
    "        n_classes=3,\n",
    "        epochs=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25ebee18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NightMare\\anaconda3\\envs\\envGPU\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "classifier.preparation(\n",
    "        X_train=list(train_data['text'][:int(1 * len(train_data))]),\n",
    "        y_train=list(train_data['label'][:int(1 * len(train_data))]),\n",
    "        X_valid=list(valid_data['text'][:int(1 * len(valid_data))]),\n",
    "        y_valid=list(valid_data['label'][:int(1 * len(valid_data))])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7b2b5b8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/103 [00:00<?, ?it/s]C:\\Temp\\ipykernel_14876\\4148355292.py:21: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  logpt = F.log_softmax(input)\n",
      "Training:   0%|          | 0/103 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mclassifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 109\u001b[0m, in \u001b[0;36mBertClassifier.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs):\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 109\u001b[0m     train_acc, train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m accuracy \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    111\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n",
      "Cell \u001b[1;32mIn[4], line 58\u001b[0m, in \u001b[0;36mBertClassifier.fit\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     54\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fn(outputs\u001b[38;5;241m.\u001b[39mlogits, targets)\n\u001b[0;32m     56\u001b[0m correct_predictions \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(preds \u001b[38;5;241m==\u001b[39m targets)\n\u001b[1;32m---> 58\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     60\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     61\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "classifier.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9584c50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = list(test_data['text'])\n",
    "labels = list(test_data['label'])\n",
    "\n",
    "predictions = [classifier.predict(t) for t in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e8bf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "_, _, f1score = precision_recall_fscore_support(labels, predictions, average='micro')[:3]\n",
    "\n",
    "print(f'f1score: {f1score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33797d83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
